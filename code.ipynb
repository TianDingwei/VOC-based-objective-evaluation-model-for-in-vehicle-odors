{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Read the data\n",
    "train_data = pd.read_excel('Train Set.xlsx', sheet_name='Interiors')\n",
    "test_data = pd.read_excel('Validation Set.xlsx', sheet_name='Interiors')\n",
    "val_data = pd.read_excel('Test Set.xlsx',sheet_name='Interiors')\n",
    "\n",
    "X_train = train_data.iloc[:, :-1]\n",
    "y_train = train_data.iloc[:, -1]\n",
    "X_test = test_data.iloc[:, :-1]\n",
    "y_test = test_data.iloc[:, -1]\n",
    "X_val = val_data.iloc[:, :-1]\n",
    "y_val = val_data.iloc[:, -1]\n",
    "\n",
    "# Create a LabelEncoder instance\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder to the training data and then transform the training, validation, and test data\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "y_val = encoder.transform(y_val)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and then transform the training, validation, and test data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Convert DataFrame to Tensor\n",
    "X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(9, 11),  # First layer\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(11, 22),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(22, 6),  # Third layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "# Use the validation set to tune the model hyperparameters\n",
    "net = Net()  \n",
    "optimizer = optim.AdamW(net.parameters(), lr=1e-2, weight_decay=0.1)  # Optimizer needs to be reinitialized as well\n",
    "\n",
    "for epoch in range(1000):  # loop over the dataset multiple times\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    if epoch % 10 == 0:\n",
    "        val_outputs = net(X_val)\n",
    "        _, predicted = torch.max(val_outputs.data, 1)\n",
    "        correct = (predicted == y_val).type(torch.float).sum().item()\n",
    "        accuracy = correct / len(y_val)\n",
    "\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_model_state = net.state_dict()\n",
    "\n",
    "        if accuracy > 0.7:\n",
    "            print('Early stopping at epoch: ', epoch)\n",
    "            break\n",
    "\n",
    "# Load the model with the highest accuracy\n",
    "net.load_state_dict(best_model_state)\n",
    "\n",
    "# Compute the accuracy\n",
    "train_outputs = net(X_train)\n",
    "_, predicted = torch.max(train_outputs.data, 1)\n",
    "train_acc = accuracy_score(y_train, predicted)\n",
    "\n",
    "test_outputs = net(X_test)                        \n",
    "_, predicted = torch.max(test_outputs.data, 1)\n",
    "test_acc = accuracy_score(y_test, predicted)\n",
    "\n",
    "val_outputs = net(X_val)\n",
    "_, predicted = torch.max(val_outputs.data, 1)\n",
    "val_acc = accuracy_score(y_val, predicted)\n",
    "\n",
    "print('Train Accuracy: ', train_acc)\n",
    "print('Validation Accuracy: ', val_acc)\n",
    "print('Test Accuracy: ', test_acc)\n",
    "\n",
    "\n",
    "# Print true and predicted labels for the validation set\n",
    "print('Validation True: ', encoder.inverse_transform(y_val))\n",
    "print('Validation Predicted: ', encoder.inverse_transform(predicted.numpy()))\n",
    "\n",
    "# Print true and predicted labels for the test set\n",
    "print('Test True: ', encoder.inverse_transform(y_test))\n",
    "print('Test Predicted: ', encoder.inverse_transform(predicted.numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Read the data\n",
    "train_data = pd.read_excel('Train Set.xlsx', sheet_name='Interiors')\n",
    "test_data = pd.read_excel('Validation Set.xlsx', sheet_name='Interiors')\n",
    "val_data = pd.read_excel('Test Set.xlsx',sheet_name='Interiors')\n",
    "\n",
    "X_train = train_data.iloc[:, :-1]\n",
    "y_train = train_data.iloc[:, -1]\n",
    "X_val = val_data.iloc[:, :-1]\n",
    "y_val = val_data.iloc[:, -1]\n",
    "X_test = test_data.iloc[:, :-1]\n",
    "y_test = test_data.iloc[:, -1]\n",
    "\n",
    "# Create a LabelEncoder instance\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder to the training data and then transform the training, validation, and test data\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_val = encoder.transform(y_val)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and then transform the training, validation, and test data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA instance\n",
    "pca = PCA(n_components=3)  # Here we are assuming we are reducing the data to 2 dimensions\n",
    "\n",
    "# Fit PCA to the training data and then transform the training, validation, and test data\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_val = pca.transform(X_val)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'] # Here we are specifying four kernel functions\n",
    "}\n",
    "\n",
    "# Create an SVM model instance\n",
    "svm_model = svm.SVC()\n",
    "\n",
    "# Create a GridSearchCV instance\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print('Best parameters: ', best_params)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict the training set\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "\n",
    "# Calculate the training accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print('SVM Model Train Accuracy: {:.2f}%'.format(train_accuracy * 100))\n",
    "\n",
    "# Predict the validation set\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Calculate the validation accuracy\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print('SVM Model Validation Accuracy: {:.2f}%'.format(val_accuracy * 100))\n",
    "\n",
    "# Predict the test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the test accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('SVM Model Test Accuracy: {:.2f}%'.format(test_accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XG-Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Read the data\n",
    "train_data = pd.read_excel('Train Set.xlsx', sheet_name='Interiors')\n",
    "test_data = pd.read_excel('Validation Set.xlsx', sheet_name='Interiors')\n",
    "val_data = pd.read_excel('Test Set.xlsx',sheet_name='Interiors')\n",
    "\n",
    "\n",
    "X_train = train_data.iloc[:, :-1]\n",
    "y_train = train_data.iloc[:, -1]\n",
    "X_val = val_data.iloc[:, :-1]\n",
    "y_val = val_data.iloc[:, -1]\n",
    "X_test = test_data.iloc[:, :-1]\n",
    "y_test = test_data.iloc[:, -1]\n",
    "\n",
    "# Create a LabelEncoder instance\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder to the training data and then transform the training, validation, and test data\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_val = encoder.transform(y_val)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and then transform the training, validation, and test data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200, 500],\n",
    "    'max_depth': [None, 3, 5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "# Create an XGBoost model instance\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# Create a GridSearchCV instance\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print('Best parameters: ', best_params)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict the training set\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "\n",
    "# Calculate the training accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print('XGBoost Model Train Accuracy: {:.2f}%'.format(train_accuracy * 100))\n",
    "\n",
    "# Predict the validation set\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Calculate the validation accuracy\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print('XGBoost Model Validation Accuracy: {:.2f}%'.format(val_accuracy * 100))\n",
    "\n",
    "# Predict the test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the test accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('XGBoost Model Test Accuracy: {:.2f}%'.format(test_accuracy * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} for training.\")\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_excel('Train Set.xlsx', sheet_name='Interiors')\n",
    "test_data = pd.read_excel('Validation Set.xlsx', sheet_name='Interiors')\n",
    "val_data = pd.read_excel('Test Set.xlsx',sheet_name='Interiors')\n",
    "\n",
    "X_train = train_data.iloc[:, :-1]\n",
    "y_train = train_data.iloc[:, -1]\n",
    "X_val = val_data.iloc[:, :-1]\n",
    "y_val = val_data.iloc[:, -1]\n",
    "X_test = test_data.iloc[:, :-1]\n",
    "y_test = test_data.iloc[:, -1]\n",
    "\n",
    "# Label encoding\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_val = encoder.transform(y_val)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "# Check for NaN or Inf values\n",
    "assert not np.any(np.isnan(X_train))\n",
    "assert not np.any(np.isnan(X_test))\n",
    "assert not np.any(np.isinf(X_train))\n",
    "assert not np.any(np.isinf(X_test))\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_val = torch.tensor(X_val.values, dtype=torch.float).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, num_samples):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(64, 64)\n",
    "        self.dropout = nn.Dropout(0.2)  # Add Dropout layer\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=num_samples, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(9 * 2 * num_samples, 150)\n",
    "        self.fc2 = nn.Linear(150, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)  # Apply Dropout layer here\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Get the number of samples in the training set\n",
    "num_samples = X_train.shape[0]\n",
    "\n",
    "# Instantiate the CNN-LSTM model with the number of samples\n",
    "net = CNNLSTM(num_samples).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(500):\n",
    "    net.train()  # Switch to train mode\n",
    "    optimizer = optim.AdamW(net.parameters(), lr=1e-3, weight_decay=0.1)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print statistics\n",
    "    if epoch % 50 == 0:\n",
    "        net.eval()  # Switch to evaluate mode\n",
    "        val_outputs = net(X_val)\n",
    "        _, predicted = torch.max(val_outputs.data, 1)\n",
    "        correct = (predicted == y_val).type(torch.float).sum().item()\n",
    "        accuracy = correct / len(y_val)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Loss: {loss.item():.4f}, Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        if accuracy > best_val_acc:\n",
    "            best_val_acc = accuracy\n",
    "            # Save the best model state here\n",
    "            best_model_state = net.state_dict()\n",
    "\n",
    "        if accuracy > 0.8:\n",
    "            print('Early stopping at Epoch: ', epoch)\n",
    "            break\n",
    "\n",
    "# Load the best model state after all epochs\n",
    "if best_model_state is not None:\n",
    "    net.load_state_dict(best_model_state)\n",
    "\n",
    "# Evaluate the model\n",
    "test_outputs = net(X_test)\n",
    "_, predicted = torch.max(test_outputs.data, 1)\n",
    "test_acc = accuracy_score(y_test.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "print('Test Accuracy: ', test_acc)\n",
    "print('Test Set Predicted Labels: ', encoder.inverse_transform(predicted.cpu().numpy()))\n",
    "print('Test Set True Labels: ', encoder.inverse_transform(y_test.cpu().numpy()))\n",
    "\n",
    "# Validation\n",
    "train_outputs = net(X_train)\n",
    "_, predicted = torch.max(train_outputs.data, 1)\n",
    "train_acc = accuracy_score(y_train.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "print('Train Accuracy: ', train_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the model state, make sure to load it on CPU\n",
    "model_state = torch.load('best_model_state.pth', map_location='cpu')\n",
    "\n",
    "# Apply the model state\n",
    "net.load_state_dict(model_state)\n",
    "net.eval()\n",
    "net.to('cpu')\n",
    "\n",
    "features = train_data.iloc[:, :-1]\n",
    "\n",
    "# Explicitly specify the feature names used during training\n",
    "feature_names = features.columns.tolist()\n",
    "\n",
    "# Create a function to wrap your model so that it can accept numpy arrays as input and return numpy arrays as output\n",
    "def predict_fn(x):\n",
    "    # Convert the input data to PyTorch tensor and make sure to compute on CPU\n",
    "    x = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "    with torch.no_grad():\n",
    "        outputs = net(x)\n",
    "    return outputs.numpy()\n",
    "\n",
    "# Make sure X_train is already on CPU\n",
    "X_train_cpu = X_train.cpu().numpy()\n",
    "\n",
    "# Create a SHAP explainer\n",
    "explainer = shap.KernelExplainer(predict_fn, X_train_cpu, feature_names=feature_names)\n",
    "\n",
    "# Select an instance to explain\n",
    "i = 11\n",
    "shap_values = explainer.shap_values(X_test.cpu().numpy()[i])\n",
    "\n",
    "# Print the instance's values and label\n",
    "print(\"Instance values:\", [round(num, 2) for num in X_test.cpu().numpy()[i]])\n",
    "print(\"Instance label:\", encoder.inverse_transform([y_test.cpu().numpy()[i]]))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Set numpy printing options to keep 2 decimal places\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "# Print the shap_values\n",
    "for class_index, shap_array in enumerate(shap_values):\n",
    "    print(f\"SHAP values for class {class_index}:\")\n",
    "    print(shap_array)\n",
    "\n",
    "# Create a function to wrap your model so that it can accept numpy arrays as input and return numpy arrays as output\n",
    "def predict(input):\n",
    "    # Convert the input data to PyTorch tensor and make sure to compute on CPU\n",
    "    input_tensor = torch.tensor(input, dtype=torch.float)\n",
    "    with torch.no_grad():\n",
    "        output = net(input_tensor)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        return predicted.numpy()\n",
    "\n",
    "\n",
    "# Create a SHAP explainer and associate feature names\n",
    "explainer = shap.Explainer(predict, X_train_cpu, feature_names=feature_names)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure X_test and X_val are already on CPU\n",
    "X_test_cpu = X_test.cpu().numpy()\n",
    "X_val_cpu = X_val.cpu().numpy()\n",
    "\n",
    "# Concatenate the test and validation set\n",
    "X_combined = np.concatenate((X_test_cpu, X_val_cpu), axis=0)\n",
    "\n",
    "# Calculate SHAP values for the concatenated dataset\n",
    "shap_values_combined = explainer(X_combined)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# Create a new figure, set DPI to 600\n",
    "plt.figure(dpi=600)\n",
    "\n",
    "# Plot SHAP summary plot on the new figure\n",
    "shap.summary_plot(shap_values_combined, X_combined, feature_names=feature_names)\n",
    "\n",
    "# Create another new figure, set DPI to 600\n",
    "plt.figure(dpi=600)\n",
    "\n",
    "# Plot SHAP heatmap on the new figure\n",
    "shap.plots.heatmap(shap_values_combined)\n",
    "\n",
    "# Display all the plots\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
